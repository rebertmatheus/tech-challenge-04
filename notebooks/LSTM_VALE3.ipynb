{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYWiPLEzxTK6"
      },
      "source": [
        "# ImportaÃ§Ã£o das dependÃªncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cw-iljfvq5jA",
        "outputId": "1da48675-2aed-41e5-8c8a-e36ea642cf79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.9.1'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import platform\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import glob\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbeQCtx6wpYB",
        "outputId": "a387a0c7-ec26-4829-906e-2d5050273130"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x111bdc170>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs5towyYbGiR"
      },
      "source": [
        "# ConfiguraÃ§Ã£o Inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "curr_ticker = \"VALE3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hecqZQ-hFLu0",
        "outputId": "044ca377-ba4f-4ab9-f49c-7b4fac1c4450"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, ticker: str):\n",
        "        # Carregar do JSON\n",
        "        config_path = f'configs/{ticker}.json'\n",
        "        with open(config_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Atribuir todos os valores do JSON\n",
        "        for key, value in data.items():\n",
        "            setattr(self, key, value)\n",
        "        \n",
        "        # Valores calculados/dinÃ¢micos/Fixos (nÃ£o vÃ£o no JSON)\n",
        "        self.TICKER = ticker\n",
        "        self.INPUT_SIZE = len(self.FEATURE_COLS)\n",
        "        self.CHECKPOINT_DIR = \"checkpoints\"\n",
        "        self.MIN_LOSS = np.inf\n",
        "        self.NUM_WORKERS = 0 if platform.system() == \"Darwin\" else 2\n",
        "        self.ACCELERATOR = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available()\n",
        "            else \"mps\" if torch.backends.mps.is_available()\n",
        "            else \"cpu\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Config carregado: VALE3\n",
            "   Features: 23\n",
            "   Sequence length: 70\n",
            "   Batch size: 16\n",
            "   Accelerator: mps\n"
          ]
        }
      ],
      "source": [
        "config = Config(curr_ticker)\n",
        "\n",
        "print(f\"âœ… Config carregado: {config.TICKER}\")\n",
        "print(f\"   Features: {len(config.FEATURE_COLS)}\")\n",
        "print(f\"   Sequence length: {config.SEQUENCE_LENGTH}\")\n",
        "print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"   Accelerator: {config.ACCELERATOR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def drop_columns(df):\n",
        "    return df.drop(columns=config.DROP_COLUMNS, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra3kOM0Bw7hX",
        "outputId": "162c1421-3805-4b48-db9a-2690878f11ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1928, 30)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "full_df = pd.read_parquet(f'data/train/{config.TICKER}.parquet')\n",
        "full_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_df = full_df.tail(config.DF_SIZE).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g8r1_5jxZ_3"
      },
      "source": [
        "# Tratamento dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0FbMr13MbeR",
        "outputId": "c6a06fcc-cbd5-46d9-af4a-c947d2b78216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset criado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset com FEATURES E TARGET normalizados\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, sequence_length, feature_cols, target_col, feature_scaler=None, target_scaler=None, fit_scalers=True):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.feature_cols = feature_cols\n",
        "        self.target_col = target_col\n",
        "\n",
        "        # Normalizar FEATURES\n",
        "        if fit_scalers:\n",
        "            self.feature_scaler = MinMaxScaler()\n",
        "            scaled_features = self.feature_scaler.fit_transform(df[feature_cols])\n",
        "        else:\n",
        "            self.feature_scaler = feature_scaler\n",
        "            scaled_features = self.feature_scaler.transform(df[feature_cols])\n",
        "\n",
        "        # Normalizar TARGET\n",
        "        if fit_scalers:\n",
        "            self.target_scaler = MinMaxScaler()\n",
        "            scaled_targets = self.target_scaler.fit_transform(df[[target_col]])\n",
        "        else:\n",
        "            self.target_scaler = target_scaler\n",
        "            scaled_targets = self.target_scaler.transform(df[[target_col]])\n",
        "\n",
        "        self.features = scaled_features\n",
        "        self.targets = scaled_targets.squeeze()\n",
        "\n",
        "        # Criar sequÃªncias\n",
        "        self.X, self.y = self._create_sequences()\n",
        "\n",
        "    def _create_sequences(self):\n",
        "        X, y = [], []\n",
        "        for i in range(self.sequence_length, len(self.features)):\n",
        "            X.append(self.features[i - self.sequence_length:i])\n",
        "            y.append(self.targets[i])\n",
        "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "    def get_scalers(self):\n",
        "        return self.feature_scaler, self.target_scaler\n",
        "\n",
        "\n",
        "print(\"Dataset criado com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StocksDataModule(pl.LightningDataModule):\n",
        "    @staticmethod\n",
        "    def drop_columns(df, columns):\n",
        "        return df.drop(columns=columns, errors='ignore')\n",
        "\n",
        "    def __init__(self, df, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.df = self.drop_columns(df, self.config.DROP_COLUMNS)\n",
        "\n",
        "        # Scalers a serem ajustados no setup\n",
        "        self.feature_scaler = None\n",
        "        self.target_scaler = None\n",
        "\n",
        "        # Datasets\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.test_dataset = None\n",
        "    \n",
        "    def setup(self, stage=None):\n",
        "        config = self.config\n",
        "        df = self.df\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PREPARANDO DADOS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Calcular Ã­ndices de split (temporal, nÃ£o aleatÃ³rio!)\n",
        "        n_samples = len(df) - config.SEQUENCE_LENGTH\n",
        "        train_end = int(n_samples * config.TRAIN_RATIO)\n",
        "        val_end = int(n_samples * (config.TRAIN_RATIO + config.VAL_RATIO))\n",
        "\n",
        "        print(f\"\\nðŸ“Š Dataset original: {len(df)} registros\")\n",
        "        print(f\"   ApÃ³s criar sequÃªncias: {n_samples} samples\")\n",
        "        print(f\"   Train: {train_end} samples ({config.TRAIN_RATIO*100:.0f}%)\")\n",
        "        print(f\"   Val: {val_end - train_end} samples ({config.VAL_RATIO*100:.0f}%)\")\n",
        "\n",
        "        print(f\"\\nðŸ“Š Splits: Train={train_end}, Val={val_end-train_end}, Test={n_samples-val_end}\")\n",
        "\n",
        "        # Criar dataset de TREINO\n",
        "        self.train_dataset = SequenceDataset(\n",
        "            df=df,\n",
        "            sequence_length=config.SEQUENCE_LENGTH,\n",
        "            feature_cols=config.FEATURE_COLS,\n",
        "            target_col=config.TARGET_COL,\n",
        "            feature_scaler=None,\n",
        "            target_scaler=None,\n",
        "            fit_scalers=True\n",
        "        )\n",
        "\n",
        "        # Salvar os scalers\n",
        "        self.feature_scaler, self.target_scaler = self.train_dataset.get_scalers()\n",
        "\n",
        "        # Criar datasets de VAL e TEST (usam os mesmos scalers)\n",
        "        self.val_dataset = SequenceDataset(\n",
        "            df=df,\n",
        "            sequence_length=config.SEQUENCE_LENGTH,\n",
        "            feature_cols=config.FEATURE_COLS,\n",
        "            target_col=config.TARGET_COL,\n",
        "            feature_scaler=self.feature_scaler,\n",
        "            target_scaler=self.target_scaler,\n",
        "            fit_scalers=False\n",
        "        )\n",
        "\n",
        "        self.test_dataset = SequenceDataset(\n",
        "            df=df,\n",
        "            sequence_length=config.SEQUENCE_LENGTH,\n",
        "            feature_cols=config.FEATURE_COLS,\n",
        "            target_col=config.TARGET_COL,\n",
        "            feature_scaler=self.feature_scaler,\n",
        "            target_scaler=self.target_scaler,\n",
        "            fit_scalers=False\n",
        "        )\n",
        "\n",
        "        # Split temporal\n",
        "        self.train_dataset.X = self.train_dataset.X[:train_end]\n",
        "        self.train_dataset.y = self.train_dataset.y[:train_end]\n",
        "        \n",
        "        self.val_dataset.X = self.val_dataset.X[train_end:val_end]\n",
        "        self.val_dataset.y = self.val_dataset.y[train_end:val_end]\n",
        "        \n",
        "        self.test_dataset.X = self.test_dataset.X[val_end:]\n",
        "        self.test_dataset.y = self.test_dataset.y[val_end:]\n",
        "        \n",
        "        print(f\"âœ… Datasets prontos:\")\n",
        "        print(f\"   Train: {len(self.train_dataset)}\")\n",
        "        print(f\"   Val: {len(self.val_dataset)}\")\n",
        "        print(f\"   Test: {len(self.test_dataset)}\")\n",
        "        \n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=self.config.NUM_WORKERS\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=self.config.NUM_WORKERS\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=self.config.NUM_WORKERS\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-WaLaQ_nsG"
      },
      "source": [
        "# ConstruÃ§Ã£o da LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bUjkjqB6_yqX"
      },
      "outputs": [],
      "source": [
        "class StocksLSTM(pl.LightningModule):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.config = config\n",
        "\n",
        "    self.lstm1 = nn.LSTM(input_size=len(config.FEATURE_COLS), hidden_size=config.INIT_HIDDEN_SIZE)\n",
        "    self.lstm2 = nn.LSTM(config.INIT_HIDDEN_SIZE, config.SECOND_HIDDEN_SIZE)\n",
        "    self.lstm3 = nn.LSTM(config.SECOND_HIDDEN_SIZE, config.SECOND_HIDDEN_SIZE, dropout=config.DROPOUT_VALUE, num_layers=config.NUM_LAYERS)\n",
        "    self.dropout = nn.Dropout(p = config.DROPOUT_VALUE)\n",
        "    if hasattr(config, 'USE_FC_LAYERS') and config.USE_FC_LAYERS:\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(config.SECOND_HIDDEN_SIZE, config.FC_HIDDEN_1),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(config.FC_DROPOUT),\n",
        "          nn.Linear(config.FC_HIDDEN_1, config.FC_HIDDEN_2),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(config.FC_DROPOUT),\n",
        "          nn.Linear(config.FC_HIDDEN_2, 1)\n",
        "      )\n",
        "    else:\n",
        "      # Manter original (1 camada)\n",
        "      self.fc = nn.Linear(config.SECOND_HIDDEN_SIZE, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #(batch_size, timesteps) -> (timesteps, batch_size, dimensions)\n",
        "    # x = x.permute(1, 0).unsqueeze(2)\n",
        "\n",
        "    x = x.permute(1, 0, 2) # -> Mesma coisa da linha debaixo, de forma \"contraÃ­da\"\n",
        "    x, _ = self.lstm1(x)\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.lstm2(x)\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.lstm3(x)\n",
        "\n",
        "    x = x[-1]\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    inputs, targets = batch\n",
        "\n",
        "    outputs = self(inputs).flatten() # -> self(inputs) -> chama o forward\n",
        "    loss = nn.functional.mse_loss(outputs, targets)\n",
        "\n",
        "    self.log(\"train_loss\", loss, prog_bar=True)\n",
        "    return loss\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    inputs, targets = batch\n",
        "\n",
        "    outputs = self(inputs).flatten()\n",
        "    loss = nn.functional.mse_loss(outputs, targets)\n",
        "\n",
        "    self.log(\"val_loss\", loss, prog_bar=True)\n",
        "    return loss\n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "    inputs, targets = batch\n",
        "\n",
        "    outputs = self(inputs).flatten()\n",
        "    loss = nn.functional.mse_loss(outputs, targets)\n",
        "\n",
        "    self.log(\"test_loss\", loss, prog_bar=True)\n",
        "    return loss\n",
        "  \n",
        "  def predict_step(self, batch, batch_idx):\n",
        "    inputs, _ = batch\n",
        "    return self(inputs).flatten()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.Adam(\n",
        "      self.parameters(),\n",
        "      lr=self.config.LEARNING_RATE,\n",
        "      weight_decay=self.config.WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer,\n",
        "      mode='min',\n",
        "      factor = self.config.RLR_FACTOR,\n",
        "      patience=self.config.RLR_PATIENCE\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'optimizer': optimizer,\n",
        "      'lr_scheduler': {\n",
        "        'scheduler': scheduler,\n",
        "        'monitor': 'val_loss' # MÃ©trica que o scheduler observa\n",
        "      }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVL1BlyWSnUZ"
      },
      "source": [
        "# Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Callbacks configurados!\n"
          ]
        }
      ],
      "source": [
        "# Early Stopping - para se val_los nÃ£o melhorar\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=config.ES_PATIENCE,\n",
        "    min_delta=config.ES_MIN_DELTA,\n",
        "    mode='min',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Model Checkpoint - Salva o melhor modelo.\n",
        "checkpoint = ModelCheckpoint(\n",
        "    dirpath=config.CHECKPOINT_DIR,\n",
        "    filename='best-{epoch:02d}-{val_loss:.4f}',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=1, # -> Salva o melhor modelo\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# LR Monitor - mostra o LR atual no progresso\n",
        "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "print(\"âœ… Callbacks configurados!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Trainer configurado!\n"
          ]
        }
      ],
      "source": [
        "# Criar o Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.EPOCHS,\n",
        "    accelerator='auto', # Detecta GPU/MPS/CPU automaticamente\n",
        "    callbacks=[\n",
        "        early_stopping,\n",
        "        checkpoint,\n",
        "        lr_monitor\n",
        "    ],\n",
        "    log_every_n_steps=config.LOG_EVERY_N_STEPS,\n",
        "    gradient_clip_val=config.GRADIENT_CLIP_VAL,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer configurado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name    | Type       | Params | Mode  | FLOPs\n",
            "-------------------------------------------------------\n",
            "0 | lstm1   | LSTM       | 287 K  | train | 0    \n",
            "1 | lstm2   | LSTM       | 197 K  | train | 0    \n",
            "2 | lstm3   | LSTM       | 264 K  | train | 0    \n",
            "3 | dropout | Dropout    | 0      | train | 0    \n",
            "4 | fc      | Sequential | 10.4 K | train | 0    \n",
            "-------------------------------------------------------\n",
            "759 K     Trainable params\n",
            "0         Non-trainable params\n",
            "759 K     Total params\n",
            "3.040     Total estimated model params size (MB)\n",
            "12        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "0         Total Flops\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Pasta de checkpoints limpa!\n",
            "\n",
            "============================================================\n",
            "PREPARANDO DADOS\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Dataset original: 1500 registros\n",
            "   ApÃ³s criar sequÃªncias: 1430 samples\n",
            "   Train: 1072 samples (75%)\n",
            "   Val: 215 samples (15%)\n",
            "\n",
            "ðŸ“Š Splits: Train=1072, Val=215, Test=143\n",
            "âœ… Datasets prontos:\n",
            "   Train: 1072\n",
            "   Val: 215\n",
            "   Test: 143\n",
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/trebert/src/python/tech-challenge-04/notebooks/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "/Users/trebert/src/python/tech-challenge-04/notebooks/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.96it/s, v_num=15, train_loss=0.0545, val_loss=0.00477]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved. New best score: 0.005\n",
            "Epoch 0, global step 67: 'val_loss' reached 0.00477 (best 0.00477), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=00-val_loss=0.0048.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.63it/s, v_num=15, train_loss=0.0633, val_loss=0.00488]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1, global step 134: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 43.81it/s, v_num=15, train_loss=0.011, val_loss=0.00834]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2, global step 201: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.32it/s, v_num=15, train_loss=0.00557, val_loss=0.00552]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3, global step 268: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.21it/s, v_num=15, train_loss=0.0059, val_loss=0.00222] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.003 >= min_delta = 0.0001. New best score: 0.002\n",
            "Epoch 4, global step 335: 'val_loss' reached 0.00222 (best 0.00222), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=04-val_loss=0.0022.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 41.98it/s, v_num=15, train_loss=0.0052, val_loss=0.00379] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5, global step 402: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.94it/s, v_num=15, train_loss=0.00474, val_loss=0.000768]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.001 >= min_delta = 0.0001. New best score: 0.001\n",
            "Epoch 6, global step 469: 'val_loss' reached 0.00077 (best 0.00077), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=06-val_loss=0.0008.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.32it/s, v_num=15, train_loss=0.0035, val_loss=0.00161]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7, global step 536: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.48it/s, v_num=15, train_loss=0.00549, val_loss=0.00153]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8, global step 603: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.29it/s, v_num=15, train_loss=0.00253, val_loss=0.00141]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9, global step 670: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.30it/s, v_num=15, train_loss=0.00279, val_loss=0.000611]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric val_loss improved by 0.000 >= min_delta = 0.0001. New best score: 0.001\n",
            "Epoch 10, global step 737: 'val_loss' reached 0.00061 (best 0.00061), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=10-val_loss=0.0006.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.96it/s, v_num=15, train_loss=0.00232, val_loss=0.000651] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11, global step 804: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 43.89it/s, v_num=15, train_loss=0.00402, val_loss=0.000607]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12, global step 871: 'val_loss' reached 0.00061 (best 0.00061), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=12-val_loss=0.0006.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.87it/s, v_num=15, train_loss=0.00414, val_loss=0.000583]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13, global step 938: 'val_loss' reached 0.00058 (best 0.00058), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=13-val_loss=0.0006.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 43.36it/s, v_num=15, train_loss=0.00204, val_loss=0.000698] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14, global step 1005: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.60it/s, v_num=15, train_loss=0.00818, val_loss=0.000759]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15, global step 1072: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.36it/s, v_num=15, train_loss=0.00419, val_loss=0.00164]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16, global step 1139: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.09it/s, v_num=15, train_loss=0.00379, val_loss=0.000631]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17, global step 1206: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 45.29it/s, v_num=15, train_loss=0.00513, val_loss=0.00082]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18, global step 1273: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.09it/s, v_num=15, train_loss=0.00464, val_loss=0.000655]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19, global step 1340: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.68it/s, v_num=15, train_loss=0.00278, val_loss=0.000534] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20, global step 1407: 'val_loss' reached 0.00053 (best 0.00053), saving model to '/Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=20-val_loss=0.0005.ckpt' as top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 43.56it/s, v_num=15, train_loss=0.00129, val_loss=0.000552] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21, global step 1474: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 43.04it/s, v_num=15, train_loss=0.00302, val_loss=0.000682] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22, global step 1541: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 43.28it/s, v_num=15, train_loss=0.00377, val_loss=0.000761] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23, global step 1608: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.76it/s, v_num=15, train_loss=0.00298, val_loss=0.00375] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24, global step 1675: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.15it/s, v_num=15, train_loss=0.00219, val_loss=0.000539]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Monitored metric val_loss did not improve in the last 15 records. Best score: 0.001. Signaling Trainer to stop.\n",
            "Epoch 25, global step 1742: 'val_loss' was not in top 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 44.08it/s, v_num=15, train_loss=0.00219, val_loss=0.000539]\n"
          ]
        }
      ],
      "source": [
        "model = StocksLSTM(config)\n",
        "datamodule = StocksDataModule(df=full_df, config=config)\n",
        "\n",
        "if os.path.exists(config.CHECKPOINT_DIR):\n",
        "    shutil.rmtree(config.CHECKPOINT_DIR)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "print(\"âœ… Pasta de checkpoints limpa!\")\n",
        "\n",
        "# Treinar o modelo\n",
        "trainer.fit(model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ Checkpoints encontrados:\n",
            "   checkpoints/best-epoch=20-val_loss=0.0005.ckpt\n",
            "\n",
            "âœ… Melhor checkpoint: /Users/trebert/src/python/tech-challenge-04/notebooks/checkpoints/best-epoch=20-val_loss=0.0005.ckpt\n",
            "âœ… Modelo carregado do checkpoint!\n"
          ]
        }
      ],
      "source": [
        "# Encontrar o melhor checkpoint salvo\n",
        "checkpoints = glob.glob(f\"{config.CHECKPOINT_DIR}/*.ckpt\")\n",
        "print(f\"ðŸ“ Checkpoints encontrados:\")\n",
        "for ckpt in checkpoints:\n",
        "    print(f\"   {ckpt}\")\n",
        "\n",
        "# Pegar o mais recente (ou vocÃª pode especificar o caminho)\n",
        "best_checkpoint = checkpoint.best_model_path  # Pega do callback\n",
        "print(f\"\\nâœ… Melhor checkpoint: {best_checkpoint}\")\n",
        "\n",
        "# Carregar o melhor modelo\n",
        "model = StocksLSTM.load_from_checkpoint(best_checkpoint, config=config)\n",
        "print(f\"âœ… Modelo carregado do checkpoint!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCty-7VMNuYW"
      },
      "source": [
        "# Validando e Testando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dados prontos para avaliaÃ§Ã£o!\n",
            "   Val samples: 215\n",
            "   Test samples: 143\n"
          ]
        }
      ],
      "source": [
        "# ObtenÃ§Ã£o dos dataloaders do datamodule\n",
        "val_loader = datamodule.val_dataloader()\n",
        "test_loader = datamodule.test_dataloader()\n",
        "\n",
        "# Obter os targets\n",
        "y_val = datamodule.val_dataset.y\n",
        "y_test = datamodule.test_dataset.y\n",
        "\n",
        "# Obter os Scalers \n",
        "target_scaler = datamodule.target_scaler\n",
        "\n",
        "print(f\"âœ… Dados prontos para avaliaÃ§Ã£o!\")\n",
        "print(f\"   Val samples: {len(y_val)}\")\n",
        "print(f\"   Test samples: {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/trebert/src/python/tech-challenge-04/notebooks/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PREPARANDO DADOS\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Dataset original: 1500 registros\n",
            "   ApÃ³s criar sequÃªncias: 1430 samples\n",
            "   Train: 1072 samples (75%)\n",
            "   Val: 215 samples (15%)\n",
            "\n",
            "ðŸ“Š Splits: Train=1072, Val=215, Test=143\n",
            "âœ… Datasets prontos:\n",
            "   Train: 1072\n",
            "   Val: 215\n",
            "   Test: 143\n",
            "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 117.75it/s]\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "     Validate metric           DataLoader 0\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "        val_loss           0.0005340442876331508\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'val_loss': 0.0005340442876331508}]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.validate(model, datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/trebert/src/python/tech-challenge-04/notebooks/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PREPARANDO DADOS\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Dataset original: 1500 registros\n",
            "   ApÃ³s criar sequÃªncias: 1430 samples\n",
            "   Train: 1072 samples (75%)\n",
            "   Val: 215 samples (15%)\n",
            "\n",
            "ðŸ“Š Splits: Train=1072, Val=215, Test=143\n",
            "âœ… Datasets prontos:\n",
            "   Train: 1072\n",
            "   Val: 215\n",
            "   Test: 143\n",
            "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 14.24it/s] \n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "       Test metric             DataLoader 0\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "        test_loss          0.0005022024852223694\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 0.0005022024852223694}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(model, datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/trebert/src/python/tech-challenge-04/notebooks/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PREPARANDO DADOS\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Dataset original: 1500 registros\n",
            "   ApÃ³s criar sequÃªncias: 1430 samples\n",
            "   Train: 1072 samples (75%)\n",
            "   Val: 215 samples (15%)\n",
            "\n",
            "ðŸ“Š Splits: Train=1072, Val=215, Test=143\n",
            "âœ… Datasets prontos:\n",
            "   Train: 1072\n",
            "   Val: 215\n",
            "   Test: 143\n",
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 78.09it/s]\n",
            "\n",
            "============================================================\n",
            "PREPARANDO DADOS\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Dataset original: 1500 registros\n",
            "   ApÃ³s criar sequÃªncias: 1430 samples\n",
            "   Train: 1072 samples (75%)\n",
            "   Val: 215 samples (15%)\n",
            "\n",
            "ðŸ“Š Splits: Train=1072, Val=215, Test=143\n",
            "âœ… Datasets prontos:\n",
            "   Train: 1072\n",
            "   Val: 215\n",
            "   Test: 143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/trebert/src/python/tech-challenge-04/notebooks/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 99.55it/s] \n",
            "\n",
            "============================================================\n",
            "âœ… PrevisÃµes geradas!\n",
            "============================================================\n",
            "   Val shape: (215, 1)\n",
            "   Test shape: (143, 1)\n"
          ]
        }
      ],
      "source": [
        "# Gerar previsÃµes usando o padrÃ£o Lightning\n",
        "predictions_val = trainer.predict(model, dataloaders=val_loader)\n",
        "predictions_test = trainer.predict(model, dataloaders=test_loader)\n",
        "\n",
        "# O trainer.predict() retorna uma LISTA de tensores (um por batch)\n",
        "# Precisamos concatenar em um Ãºnico array\n",
        "predictions_val = torch.cat(predictions_val).numpy().reshape(-1, 1)\n",
        "predictions_test = torch.cat(predictions_test).numpy().reshape(-1, 1)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"âœ… PrevisÃµes geradas!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"   Val shape: {predictions_val.shape}\")\n",
        "print(f\"   Test shape: {predictions_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "âœ… Valores convertidos para escala real!\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š PrevisÃµes (teste):\n",
            "   Min: R$ 45.58\n",
            "   Max: R$ 62.16\n",
            "\n",
            "ðŸ“Š Valores reais (teste):\n",
            "   Min: R$ 48.23\n",
            "   Max: R$ 63.81\n"
          ]
        }
      ],
      "source": [
        "predictions_val_real = target_scaler.inverse_transform(predictions_val)\n",
        "predictions_test_real = target_scaler.inverse_transform(predictions_test)\n",
        "\n",
        "# Converter targets para valores reais tambÃ©m\n",
        "y_val_real = target_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
        "y_test_real = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"âœ… Valores convertidos para escala real!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nðŸ“Š PrevisÃµes (teste):\")\n",
        "print(f\"   Min: R$ {predictions_test_real.min():.2f}\")\n",
        "print(f\"   Max: R$ {predictions_test_real.max():.2f}\")\n",
        "print(f\"\\nðŸ“Š Valores reais (teste):\")\n",
        "print(f\"   Min: R$ {y_test_real.min():.2f}\")\n",
        "print(f\"   Max: R$ {y_test_real.max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ucf_uacleqY"
      },
      "source": [
        "# AvaliaÃ§Ã£o do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcular_metricas(y_real, predictions, nome_conjunto):\n",
        "    \"\"\"Calcula e exibe mÃ©tricas para um conjunto de dados.\"\"\"\n",
        "    mae = mean_absolute_error(y_real, predictions)\n",
        "    mse = mean_squared_error(y_real, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_real, predictions)\n",
        "    \n",
        "    # MAPE - Mean Absolute Percentage Error\n",
        "    # Evita divisÃ£o por zero usando np.where\n",
        "    mape = np.mean(np.abs((y_real - predictions) / np.where(y_real != 0, y_real, 1))) * 100\n",
        "    \n",
        "    # AcurÃ¡cia Direcional\n",
        "    mean_y = np.mean(y_real)\n",
        "    real_direction = np.sign(y_real - mean_y)\n",
        "    pred_direction = np.sign(predictions - mean_y)\n",
        "    directional_accuracy = (real_direction == pred_direction).mean()\n",
        "    \n",
        "    print(f\"\\nðŸ“Š {nome_conjunto}:\")\n",
        "    print(f\"   MAE:  R$ {mae:.3f}\")\n",
        "    print(f\"   MSE:  R$ {mse:.3f}\")\n",
        "    print(f\"   RMSE: R$ {rmse:3f}\")\n",
        "    print(f\"   MAPE: {mape:.2f}%\")\n",
        "    print(f\"   RÂ²:   {r2:.3f} ({r2*100:.2f}%)\")\n",
        "    print(f\"   AcurÃ¡cia Direcional: {directional_accuracy*100:.2f}%\")\n",
        "    \n",
        "    return {'mae': mae, 'rmse': rmse, 'mape': mape, 'r2': r2, 'da': directional_accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "AVALIAÃ‡ÃƒO DO MODELO - VALE3\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š VALIDAÃ‡ÃƒO:\n",
            "   MAE:  R$ 0.993\n",
            "   MSE:  R$ 1.731\n",
            "   RMSE: R$ 1.315861\n",
            "   MAPE: 1.86%\n",
            "   RÂ²:   0.719 (71.89%)\n",
            "   AcurÃ¡cia Direcional: 76.74%\n",
            "\n",
            "ðŸ“Š TESTE:\n",
            "   MAE:  R$ 0.930\n",
            "   MSE:  R$ 1.628\n",
            "   RMSE: R$ 1.276030\n",
            "   MAPE: 1.73%\n",
            "   RÂ²:   0.862 (86.24%)\n",
            "   AcurÃ¡cia Direcional: 95.10%\n",
            "\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "OBSERVAÃ‡Ã•ES\n",
            "============================================================\n",
            "Se teste â‰ˆ validaÃ§Ã£o â†’ Modelo generaliza bem âœ…\n",
            "Se teste << validaÃ§Ã£o â†’ PossÃ­vel overfitting âš ï¸\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"AVALIAÃ‡ÃƒO DO MODELO - {config.TICKER}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "metricas_val = calcular_metricas(y_val_real, predictions_val_real, \"VALIDAÃ‡ÃƒO\")\n",
        "metricas_test = calcular_metricas(y_test_real, predictions_test_real, \"TESTE\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"OBSERVAÃ‡Ã•ES\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Se teste â‰ˆ validaÃ§Ã£o â†’ Modelo generaliza bem âœ…\")\n",
        "print(f\"Se teste << validaÃ§Ã£o â†’ PossÃ­vel overfitting âš ï¸\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GrafÃ­cos de ValidaÃ§Ã£o e Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# GrÃ¡fico 1: PrevisÃµes vs Reais (Teste) - Linha do tempo\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(y_test_real, label='Real', color='blue', alpha=0.7)\n",
        "ax1.plot(predictions_test_real, label='PrevisÃ£o', color='red', alpha=0.7)\n",
        "ax1.set_title(f'PrevisÃµes vs Reais - Teste ({config.TICKER})')\n",
        "ax1.set_xlabel('Amostras')\n",
        "ax1.set_ylabel('PreÃ§o (R$)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# GrÃ¡fico 2: PrevisÃµes vs Reais (ValidaÃ§Ã£o) - Linha do tempo\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(y_val_real, label='Real', color='blue', alpha=0.7)\n",
        "ax2.plot(predictions_val_real, label='PrevisÃ£o', color='red', alpha=0.7)\n",
        "ax2.set_title(f'PrevisÃµes vs Reais - ValidaÃ§Ã£o ({config.TICKER})')\n",
        "ax2.set_xlabel('Amostras')\n",
        "ax2.set_ylabel('PreÃ§o (R$)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# GrÃ¡fico 3: Scatter plot (Teste)\n",
        "ax3 = axes[1, 0]\n",
        "ax3.scatter(y_test_real, predictions_test_real, alpha=0.5, color='green')\n",
        "min_val = min(y_test_real.min(), predictions_test_real.min())\n",
        "max_val = max(y_test_real.max(), predictions_test_real.max())\n",
        "ax3.plot([min_val, max_val], [min_val, max_val], 'k--', label='Ideal (y=x)')\n",
        "ax3.set_title('DispersÃ£o - Teste')\n",
        "ax3.set_xlabel('Valor Real (R$)')\n",
        "ax3.set_ylabel('PrevisÃ£o (R$)')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# GrÃ¡fico 4: Scatter plot (ValidaÃ§Ã£o)\n",
        "ax4 = axes[1, 1]\n",
        "ax4.scatter(y_val_real, predictions_val_real, alpha=0.5, color='orange')\n",
        "min_val = min(y_val_real.min(), predictions_val_real.min())\n",
        "max_val = max(y_val_real.max(), predictions_val_real.max())\n",
        "ax4.plot([min_val, max_val], [min_val, max_val], 'k--', label='Ideal (y=x)')\n",
        "ax4.set_title('DispersÃ£o - ValidaÃ§Ã£o')\n",
        "ax4.set_xlabel('Valor Real (R$)')\n",
        "ax4.set_ylabel('PrevisÃ£o (R$)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analise da baixa Performance do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DIAGNÃ“STICO: Por que RÂ² estÃ¡ negativo?\n",
        "# ============================================================\n",
        "\n",
        "print(\"ðŸ“Š ANÃLISE DO SCALER (TARGET)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Range que o scaler aprendeu no TREINO\n",
        "train_min = target_scaler.data_min_[0]\n",
        "train_max = target_scaler.data_max_[0]\n",
        "print(f\"\\nðŸ”µ Range do TREINO (usado para normalizar):\")\n",
        "print(f\"   Min: R$ {train_min:.2f}\")\n",
        "print(f\"   Max: R$ {train_max:.2f}\")\n",
        "\n",
        "print(f\"\\nðŸŸ  Range da VALIDAÃ‡ÃƒO (real):\")\n",
        "print(f\"   Min: R$ {y_val_real.min():.2f}\")\n",
        "print(f\"   Max: R$ {y_val_real.max():.2f}\")\n",
        "\n",
        "print(f\"\\nðŸ”´ Range do TESTE (real):\")\n",
        "print(f\"   Min: R$ {y_test_real.min():.2f}\")\n",
        "print(f\"   Max: R$ {y_test_real.max():.2f}\")\n",
        "\n",
        "# Verificar se estÃ¡ fora do range\n",
        "val_fora = (y_val_real.min() < train_min) or (y_val_real.max() > train_max)\n",
        "test_fora = (y_test_real.min() < train_min) or (y_test_real.max() > train_max)\n",
        "\n",
        "print(f\"\\nâš ï¸  PROBLEMA DETECTADO:\")\n",
        "print(f\"   ValidaÃ§Ã£o fora do range de treino: {'SIM âŒ' if val_fora else 'NÃƒO âœ…'}\")\n",
        "print(f\"   Teste fora do range de treino: {'SIM âŒ' if test_fora else 'NÃƒO âœ…'}\")\n",
        "\n",
        "# Verificar output do modelo\n",
        "print(f\"\\nðŸ“ˆ Output do modelo (normalizado 0-1):\")\n",
        "print(f\"   PrevisÃµes Val - Min: {predictions_val.min():.4f}, Max: {predictions_val.max():.4f}\")\n",
        "print(f\"   PrevisÃµes Test - Min: {predictions_test.min():.4f}, Max: {predictions_test.max():.4f}\")\n",
        "\n",
        "# O que o sigmoid limita\n",
        "print(f\"\\nðŸ”’ Sigmoid limita output entre 0 e 1\")\n",
        "print(f\"   Se target normalizado > 1, modelo NUNCA vai acertar!\")\n",
        "\n",
        "# Calcular target normalizado\n",
        "y_test_normalized = datamodule.test_dataset.y\n",
        "print(f\"\\nðŸ“Š Target TESTE (normalizado):\")\n",
        "print(f\"   Min: {y_test_normalized.min():.4f}\")\n",
        "print(f\"   Max: {y_test_normalized.max():.4f}\")\n",
        "\n",
        "if y_test_normalized.max() > 1 or y_test_normalized.min() < 0:\n",
        "    print(f\"\\nâŒ PROBLEMA: Target de teste estÃ¡ FORA do range 0-1!\")\n",
        "    print(f\"   O scaler foi ajustado no treino, mas teste tem valores diferentes.\")\n",
        "    print(f\"   Sigmoid nÃ£o consegue produzir valores > 1 ou < 0.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DEBUG: VERIFICAR DISTRIBUIÃ‡ÃƒO DOS DADOS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nðŸ“Š ANÃLISE DE DISTRIBUIÃ‡ÃƒO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Range do TARGET no treino vs teste\n",
        "train_min = datamodule.train_dataset.targets.min()\n",
        "train_max = datamodule.train_dataset.targets.max()\n",
        "test_min = datamodule.test_dataset.targets.min()\n",
        "test_max = datamodule.test_dataset.targets.max()\n",
        "\n",
        "print(f\"\\nðŸ”µ TARGET NORMALIZADO (0-1):\")\n",
        "print(f\"   Train: [{train_min:.4f}, {train_max:.4f}]\")\n",
        "print(f\"   Test:  [{test_min:.4f}, {test_max:.4f}]\")\n",
        "\n",
        "if test_min < 0 or test_max > 1:\n",
        "    print(f\"\\nâŒ PROBLEMA: Teste estÃ¡ FORA do range 0-1!\")\n",
        "    print(f\"   Scaler foi ajustado no treino, mas teste tem valores diferentes.\")\n",
        "\n",
        "# Range do TARGET em valores reais\n",
        "train_real_min = datamodule.target_scaler.data_min_[0]\n",
        "train_real_max = datamodule.target_scaler.data_max_[0]\n",
        "test_real_min = y_test_real.min()\n",
        "test_real_max = y_test_real.max()\n",
        "\n",
        "print(f\"\\nðŸŸ  TARGET REAL (R$):\")\n",
        "print(f\"   Train: [R$ {train_real_min:.2f}, R$ {train_real_max:.2f}]\")\n",
        "print(f\"   Test:  [R$ {test_real_min:.2f}, R$ {test_real_max:.2f}]\")\n",
        "\n",
        "if test_real_min < train_real_min or test_real_max > train_real_max:\n",
        "    print(f\"\\nâš ï¸  ALERTA: Teste tem valores FORA do range de treino!\")\n",
        "    print(f\"   DiferenÃ§a Min: R$ {test_real_min - train_real_min:.2f}\")\n",
        "    print(f\"   DiferenÃ§a Max: R$ {test_real_max - train_real_max:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(full_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lstm-stocks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
